{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4808ade7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "# Packages\n",
    "import gensim\n",
    "import jieba\n",
    "import zhconv\n",
    "import opencc\n",
    "from gensim.corpora import WikiCorpus\n",
    "from datetime import datetime as dt\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cdb7ce96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47ad2a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Download_addres='https://github.com/fxsjy/jieba/raw/master/extra_dict/dict.txt.big'\n",
    "requests.get(Download_addres)\n",
    "f=requests.get(Download_addres)\n",
    "#下载文件\n",
    "with open(\"dict.txt.big\",\"wb\") as code:\n",
    "     code.write(f.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "998151f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "jieba.set_dictionary('dict.txt.big')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41d6ec29",
   "metadata": {},
   "outputs": [],
   "source": [
    "ZhWiki = \"zhwiki-latest-pages-articles-multistream.xml.bz2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1233e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1dae7f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Download and installation successful\n",
      "You can now load the package via spacy.load('zh_core_web_sm')\n",
      "✔ Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "# 下載語言模組\n",
    "spacy.cli.download(\"zh_core_web_sm\")  # 下載 spacy 中文模組\n",
    "spacy.cli.download(\"en_core_web_sm\")  # 下載 spacy 英文模組\n",
    "\n",
    "nlp_zh = spacy.load(\"zh_core_web_sm\") # 載入 spacy 中文模組\n",
    "nlp_en = spacy.load(\"en_core_web_sm\") # 載入 spacy 英文模組"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7a05af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--\n",
      "\n",
      "中文停用詞 Total=1891: ['无论', '今後', '；', '曾', '［⑥］', '为止', '呆呆地', 'В', '重新', '彼时', '不得不', '成为', '普遍', '每当', '还是', '不力', '此间', '动辄', '［②ｈ］', '争取'] ...\n",
      "--\n",
      "英文停用詞 Total=326: ['three', 'still', 'take', 'nowhere', 'except', 'hereafter', 'most', 'thereafter', 'during', 'yourself', 'was', 'been', 'n’t', 'very', 'thru', '’m', 'least', 'almost', 'whereas', 'towards'] ...\n"
     ]
    }
   ],
   "source": [
    "# 印出前20個停用詞\n",
    "print('--\\n')\n",
    "print(f\"中文停用詞 Total={len(nlp_zh.Defaults.stop_words)}: {list(nlp_zh.Defaults.stop_words)[:20]} ...\")\n",
    "print(\"--\")\n",
    "print(f\"英文停用詞 Total={len(nlp_en.Defaults.stop_words)}: {list(nlp_en.Defaults.stop_words)[:20]} ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b829890c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2222\n"
     ]
    }
   ],
   "source": [
    "STOPWORDS =  nlp_zh.Defaults.stop_words | \\\n",
    "             nlp_en.Defaults.stop_words | \\\n",
    "             set([\"\\n\", \"\\r\\n\", \"\\t\", \" \", \"\"])\n",
    "print(len(STOPWORDS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be02a56f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3005\n"
     ]
    }
   ],
   "source": [
    "# 將簡體停用詞轉成繁體，擴充停用詞表\n",
    "for word in STOPWORDS.copy():\n",
    "    STOPWORDS.add(zhconv.convert(word, \"zh-tw\"))\n",
    "    \n",
    "print(len(STOPWORDS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa2ba4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_tokenize(\n",
    "    text: str, token_min_len: int=1, token_max_len: int=15, lower: bool=True) -> List[str]:\n",
    "    if lower:\n",
    "        text  = text.lower()\n",
    "    text = zhconv.convert(text, \"zh-tw\")\n",
    "    return [\n",
    "        token for token in jieba.cut(text, cut_all=False)\n",
    "        if token_min_len <= len(token) <= token_max_len and \\\n",
    "            token not in STOPWORDS\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e682d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing zhwiki-latest-pages-articles-multistream.xml.bz2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\tensorflows_env1\\lib\\site-packages\\gensim\\utils.py:1332: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected %s; aliasing chunkize to chunkize_serial\" % entity)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Parsing {ZhWiki}...\")\n",
    "wiki_corpus = WikiCorpus(ZhWiki, tokenizer_func=preprocess_and_tokenize, token_min_len=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdd5d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "WIKI_SEG_TXT = \"wiki_seg.txt\"\n",
    "\n",
    "generator = wiki_corpus.get_texts()\n",
    "\n",
    "with open(WIKI_SEG_TXT, \"w\", encoding='utf-8') as output:\n",
    "    for texts_num, tokens in enumerate(generator):\n",
    "        output.write(\" \".join(tokens) + \"\\n\")\n",
    "\n",
    "        if (texts_num + 1) % 100000 == 0:\n",
    "            print(f\"[{str(dt.now()):.19}] 已寫入 {texts_num} 篇斷詞文章\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da7f668",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "import multiprocessing\n",
    "\n",
    "max_cpu_counts = multiprocessing.cpu_count()\n",
    "word_dim_size = 300  #  設定 word vector 維度\n",
    "print(f\"Use {max_cpu_counts} workers to train Word2Vec (dim={word_dim_size})\")\n",
    "\n",
    "\n",
    "# 讀取訓練語句\n",
    "sentences = word2vec.LineSentence(WIKI_SEG_TXT)\n",
    "\n",
    "# 訓練模型\n",
    "model = word2vec.Word2Vec(sentences, size=word_dim_size, workers=max_cpu_counts)\n",
    "\n",
    "# 儲存模型\n",
    "output_model = f\"word2vec.zh.{word_dim_size}.model\"\n",
    "model.save(output_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99eeac96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
